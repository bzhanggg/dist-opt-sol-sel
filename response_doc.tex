Reviewer 3 response:

We observed little difference in the performance of both algorithms when tuning the convexity parameter $\theta$. In our initial experiment, $\theta := 10^{-5} + \max{0, -\lambda_{\min}}$. We now parameterize $\theta$ as $\theta_g := g + \max\{0, -\lambda_{\min}\}$ where $g \in \{0, 10^{-5}, 10^{-1}\}$. $g$ now dictates the convexity of the regularized global function; when $g = 0$, $f$ is merely convex. For $g > 0$, $f$ is $\theta$-strongly convex.

CHECK THE ABOVE PARAGRAPH.

When plotting each of the plots for $\theta_g, g \in \{0, 10^{-5}, 10^{-1}\}$, we do not see any visible performance difference between $\theta_0$ and $\theta_{10^{-5}}$. Furthermore, there is negligible difference as we increase $g$. In this particular experiment, therefore, we do not see any significant performance drop when relaxing the strong convexity requirement. This is an interesting outcome that could be explored in a future work.

INCLUDE PLOTS IN \texttt{./convexity-experiment/} HERE.


Reviewer 6 response:

To provide a baseline for comparison against IR-Push-Pull and IR-DSGT, we turn to an Iteratively Regularized Gradient (IRG) method first introduced in [Kaushik, Yousefian 2019] https://arxiv.org/pdf/2007.15845.

We look at a simplification of the aRB-IRG method proposed in the above paper, where a centralized computing agent has knowledge of all players' current state within the Nash game. In the $k+1$th step, the centralized agent updates the position of the $i$th player according to the following rule:
\[
x_{k+1}^{(i)} := \begin{cases}
    x_k^{(i_k)} - \gamma_k \left( F_{i_k}(x_k) + \eta_k \tilde{\lambda}_{i_k} f(x_k) \right) & \text{if } i = i_k \\
    x_{k}^{(i)} & \text{if } i \neq i_k
\end{cases}
\]
Here, $\gamma_k$ is a decreasing step size and $\lambda_k$ is a diminishing regularization parameter. As in our original experiment, we define these $\gamma_k$ and $\lambda_k$ as follows:
\[
\gamma_k := \frac{\hat{\gamma}}{(k + \Gamma)^a}, \hspace{1cm} \lambda_k := \frac{\lambda}{(k+\Gamma)^b}
\]
We used the same parameters as in our original experiment for $\hat{\gamma}$, $\lambda$, $\Gamma$, and $a, b$.

INCLUDE PLOTS IN \texttt{./alg*-*-plots/} HERE.